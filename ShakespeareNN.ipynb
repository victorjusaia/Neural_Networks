{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32c68d24-cd5a-4225-b740-dd24e2dfa826",
   "metadata": {},
   "source": [
    "Importamos las librerias Necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c111e46-1619-4676-92a3-17c98b0c02cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25e54f5-1989-4110-9cdf-9cf6cc912e85",
   "metadata": {},
   "source": [
    "Creamos una clase para almacenar todos los datos de la red neuronal. Utilizaremos la lógica de temperatura para controlar la creatividad del modelo.\n",
    "Esta logica se basa en la formula: $$\n",
    "T(x) = \\frac{\\exp(x / T)}{\\sum \\exp(x_i / T)}\n",
    "$$\n",
    " * La temperatura ajusta la distribución de probabilidad de las predicciones del modelo. \n",
    " * Cuando la temperatura es alta (>1), las probabilidades se distribuyen más uniformemente, \n",
    "   lo que da como resultado respuestas más diversas.\n",
    " * Cuando la temperatura es baja (<1), el modelo se vuelve más confiado y genera respuestas \n",
    "   más determinísticas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ef6ae39-c52f-4f60-b7c6-76d24e0b86ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGeneratorRNN:\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Construcción del modelo con Keras\n",
    "        self.model = Sequential([\n",
    "            Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
    "            SimpleRNN(hidden_size, activation='tanh', return_sequences=True),\n",
    "            Dense(vocab_size, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    def train(self, X, y, epochs=10, batch_size=32):\n",
    "        self.model.fit(X, y, epochs=epochs, batch_size=batch_size)\n",
    "    \n",
    "    def generate_text(self, seed_text, tokenizer, max_length=100, temperature=1.0):\n",
    "        generated_text = seed_text\n",
    "        for _ in range(max_length):\n",
    "            tokenized_input = tokenizer.texts_to_sequences([generated_text])\n",
    "            tokenized_input = pad_sequences(tokenized_input, maxlen=max_length, padding='pre')\n",
    "            \n",
    "            predictions = self.model.predict(tokenized_input, verbose=0)[0][-1]\n",
    "            predictions = np.log(predictions + 1e-8) / temperature\n",
    "            exp_preds = np.exp(predictions)\n",
    "            probabilities = exp_preds / np.sum(exp_preds)\n",
    "            \n",
    "            next_word_index = np.random.choice(len(probabilities), p=probabilities)\n",
    "            next_word = tokenizer.index_word.get(next_word_index, '')\n",
    "            \n",
    "            if not next_word:\n",
    "                break\n",
    "            \n",
    "            generated_text += ' ' + next_word\n",
    "        \n",
    "        return generated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dab60bc-0a11-4486-a0a5-7bb74acfd464",
   "metadata": {},
   "source": [
    "Para esta red vamos a utilizar el dataset de Shakespeare que contiene dialogos y textos de varias obras de Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fd3d6e7-74f0-48f0-b555-c538dd769e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar y preprocesar el dataset de Shakespeare\n",
    "def load_shakespeare_data(seq_length=50):\n",
    "    path = tf.keras.utils.get_file(\"shakespeare.txt\", \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().lower()\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts([text])\n",
    "    sequences = tokenizer.texts_to_sequences([text])[0]\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    input_sequences = []\n",
    "    for i in range(len(sequences) - seq_length):\n",
    "        input_sequences.append(sequences[i:i+seq_length+1])  # Ventana deslizante fija\n",
    "\n",
    "    input_sequences = np.array(input_sequences)\n",
    "    X, y = input_sequences[:, :-1], input_sequences[:, 1:]  # Desplazamos las etiquetas\n",
    "    \n",
    "    return X, y, vocab_size, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0120f1f-1075-463b-8e82-4364bc8d9bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "X, y, vocab_size, tokenizer = load_shakespeare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc64b7ca-57d8-4fc8-b627-3439ee8cd22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m6377/6377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m798s\u001b[0m 125ms/step - accuracy: 0.0849 - loss: 6.0464\n",
      "Epoch 2/10\n",
      "\u001b[1m6377/6377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m789s\u001b[0m 124ms/step - accuracy: 0.2771 - loss: 3.8413\n",
      "Epoch 3/10\n",
      "\u001b[1m6377/6377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m790s\u001b[0m 124ms/step - accuracy: 0.3841 - loss: 3.1336\n",
      "Epoch 4/10\n",
      "\u001b[1m6377/6377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m790s\u001b[0m 124ms/step - accuracy: 0.4336 - loss: 2.8336\n",
      "Epoch 5/10\n",
      "\u001b[1m6377/6377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m793s\u001b[0m 124ms/step - accuracy: 0.4626 - loss: 2.6679\n",
      "Epoch 6/10\n",
      "\u001b[1m6377/6377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m794s\u001b[0m 124ms/step - accuracy: 0.4805 - loss: 2.5669\n",
      "Epoch 7/10\n",
      "\u001b[1m6377/6377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m794s\u001b[0m 124ms/step - accuracy: 0.4939 - loss: 2.4920\n",
      "Epoch 8/10\n",
      "\u001b[1m6377/6377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m794s\u001b[0m 125ms/step - accuracy: 0.5042 - loss: 2.4363\n",
      "Epoch 9/10\n",
      "\u001b[1m6377/6377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m795s\u001b[0m 125ms/step - accuracy: 0.5115 - loss: 2.3946\n",
      "Epoch 10/10\n",
      "\u001b[1m6377/6377\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m814s\u001b[0m 128ms/step - accuracy: 0.5178 - loss: 2.3612\n"
     ]
    }
   ],
   "source": [
    "# Crear y entrenar el modelo\n",
    "model = TextGeneratorRNN(vocab_size, embedding_dim=64, hidden_size=128)\n",
    "model.train(X, y, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53ad34d8-e5a8-4aea-9434-1dd487b5896d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " to be or not to be his country page and that i were a mockery king of snow standing before the sun of love and others live sister hither widow miserable day clown then sit richard presently if you be gone away with you sir ay good sir good letters kate and be it like a\n"
     ]
    }
   ],
   "source": [
    "# Generar texto de ejemplo\n",
    "seed_text = \"to be or not to be\"\n",
    "generated_text = model.generate_text(seed_text, tokenizer, max_length=50, temperature=0.7)\n",
    "print(\"Generated Text:\\n\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a49d64-98b5-48bb-b4f6-cb70e5b48f76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
